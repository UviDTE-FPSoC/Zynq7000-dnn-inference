#include <assert.h>
#include <dirent.h>
#include <stdio.h>
#include <stdlib.h>
#include <sys/stat.h>
#include <unistd.h>
#include <cassert>
#include <cmath>
#include <cstdio>
#include <fstream>
#include <iomanip>
#include <iostream>
#include <chrono>
#include <ctime>
#include <queue>
#include <string>
#include <vector>

/* header file OpenCV for image processing */
#include <opencv2/opencv.hpp>

/* header file for DNNDK APIs */
#include <dnndk/dnndk.h>

using namespace std;
using namespace cv;

/* Name of the DPU kernel generated by the DNNC compiler */
#define KERNEL_DNN "mobilenet_v1_0"
/* Input Node for Kernel Mobilenet_v1 */
#define INPUT_NODE      "MobilenetV1_MobilenetV1_Conv2d_0_Conv2D"
/* Output Node for Kernel Mobilenet_v1 */
#define OUTPUT_NODE     "MobilenetV1_Logits_Conv2d_1c_1x1_Conv2D"
/* File where the execution time results are going to be writed to */
#define OUTPUT_FILENAME "tf_Mobilenetv1_RandomInput.txt"

/* Path to the images in the target board considering this application is stored in a folder within the ZedBoard directory. */
const string inferenceImages = "../../test_images/";


/**
 * @brief calculate the softmax layer
 *
 * @param data - pointer to input buffer
 * @param size - size of input buffer
 * @param result - pointer to a result array with the size of the ouput tensor channels
 *
 * @return none
 */
void CPUCalcSoftmax(const float *data, size_t size, float *result) {
    assert(data && result);
    double sum = 0.0f;

    /* Implementation of softmax function */
    for (size_t i = 0; i < size; i++) {
        result[i] = exp(data[i]);
        sum += result[i];
    }

    for (size_t i = 0; i < size; i++) {
        result[i] /= sum;
    }
}



/**
 * @brief Run DPU Task for the deep neural network
 *
 * @param taskDNN - pointer to the DNN Task
 *
 * @return none
 */
void runDNN(DPUTask *taskDNN) {

    /* In the task pointer doesn't contain a pointer, the function is aborted. */
    assert(taskDNN);

    /* Get size of the input tensor */
    int inputTensor_height = dpuGetInputTensorHeight(taskDNN, INPUT_NODE);
    int inputTensor_width = dpuGetInputTensorWidth(taskDNN, INPUT_NODE);

    /* Get channel count of the output Tensor for the DNN Task  */
    int channel = dpuGetOutputTensorChannel(taskDNN, OUTPUT_NODE);
    float *FCResult = new float[channel];
    float *softmax = new float[channel];

    /* Create file to store execution inference results */
    ofstream fs(OUTPUT_FILENAME);
    fs.close();


    for (int i = 0; i<500; i = i + 1) {	// Expresion to go through out all the iterations of the images vector.
        cout << "Entered loop: " << i << endl;

        /* Create random input */
        Mat input(inputTensor_width, inputTensor_height, CV_8SC(3), Scalar::all(0));
        randu(input, Scalar::all(-127), Scalar::all(127));
        
        /* Initialize and start timer */
        std::chrono::time_point<std::chrono::high_resolution_clock> start, end;
        std::chrono::duration<double> elapsed_seconds;
        start = std::chrono::high_resolution_clock::now();

        /* Set input data into DPU task */
        dpuSetInputImage2(taskDNN, INPUT_NODE, input);

        /* Launch the DNN Task */
        dpuRunTask(taskDNN);

        /* Get DPU execution time (in us) of DPU Task */
        //long long timeProf = dpuGetTaskProfile(taskDNN);
        //cout << "  DPU Task Execution time: " << (timeProf * 1.0f) << "us\n";

        /* Get FC (Fully Connected layer) result and convert from INT8 to FP32 format. This is equal to spatial squeeze */
        dpuGetOutputTensorInHWCFP32(taskDNN, OUTPUT_NODE, FCResult, channel);

        /* Calculate softmax on CPU */
        CPUCalcSoftmax(FCResult, channel, softmax);

        /* Stop the timer and calculate the execution time */
        end = std::chrono::high_resolution_clock::now();
        elapsed_seconds = end - start;

        /* Write execution time results in miliseconds to file */
        fs.open(OUTPUT_FILENAME, std::fstream::out | std::fstream::app);
        if(!fs)
        {
            cerr<<"Cannot open the output file."<<std::endl;
        } else {
            fs << elapsed_seconds.count()*1000 << "\n";
            fs.close();
            cout << "\nInput written to textfile" << endl;
        }
    }

    delete[] softmax;
    delete[] FCResult;
}




/**
 * @brief Entry for runing a deep neural network
 *
 * @note Functions prefixed with "dpu" are imported as DNNDK APIs to reduce the
 *
 *       complexity of programing a neural network application for the DPU.
 *
 */
int main(void) {
    /* DPU Kernel/Task for running the DNN. A pointer is created for both the kernel and the task with the corresponding datatype. */
    DPUKernel *kernelDNN;
    DPUTask *taskDNN;

    /* Attach and open DPU device file “/dev/dpu” before the utilization of DPU resources.
       Returns a 0 if succesfull or a negative number in case of error. */
    dpuOpen();

    /* Load DPU Kernel for the network. KERNEL_DNN contains a string with the 
       name of the kernel outputed by the DNNC compiler */
    kernelDNN = dpuLoadKernel(KERNEL_DNN);

    /* Create DPU Task for the neural network. The arguments indicated are a pointer to the 
       DPU kernel and the indication for Normal mode. */
    taskDNN = dpuCreateTask(kernelDNN, 0);

    /* Run the deep neural network Task */
    runDNN(taskDNN);

    /* Destroy DPU Task & free resources */
    dpuDestroyTask(taskDNN);

    /* Destroy DPU Kernel & free resources */
    dpuDestroyKernel(kernelDNN);

    /* Dettach from DPU driver & free resources */
    dpuClose();

    return 0;
}